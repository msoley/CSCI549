{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text analysis examples\n",
        "In this practicum, we learn to manipulate textual data and perform simple natural language processing tasks including text preprocessing, topic modeling and classification."
      ],
      "metadata": {
        "id": "I2SopsWjjY8X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aECRL66dEbR"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "sOBrPz5JdEbU",
        "outputId": "c0d3c120-9e26-49f6-915e-d799e85e4476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8a1626100f86>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# We want the exported object to be the class, so we first import the module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# to make sure a later import doesn't overwrite the class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_config_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# TODO(phawkins): fix users of this alias and delete this file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransfer_guard_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_client\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxla_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlapack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jaxlib/xla_client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxla_extension\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_xla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mml_dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: initialization failed",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pds\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n",
        "import sklearn\n",
        "import sklearn.model_selection as ms\n",
        "import sklearn.feature_extraction.text as text\n",
        "import torch\n",
        "from nltk.tree import Tree\n",
        "\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sklearn.naive_bayes as nb\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/msoley/DSCI549/master/In-class%20exercises/Practicum4/tweet_global_warming.csv\n",
        "dataset= './tweet_global_warming.csv'\n",
        "#source https://www.figure-eight.com/data-for-everyone/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mayplC5SdEbl"
      },
      "source": [
        "### NLTK\n",
        "In this section we tokenize and parse a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFoP-oE0dEbm"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSGg3JLGdEbn"
      },
      "outputs": [],
      "source": [
        "example_text = \"Penguins are flightless birds.\"\n",
        "tokenizer.tokenize(example_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YFXYSWedEbo"
      },
      "outputs": [],
      "source": [
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatizer.lemmatize('brought',pos='v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAM9DhQCdEbq"
      },
      "outputs": [],
      "source": [
        "# draw parse tree\n",
        "tree_str = \"(ROOT (S (NP (NNS Penguins)) (VP (VBP are) (NP (JJ flightless) (NNS birds))) (. .)))\"\n",
        "this_tree = Tree.fromstring(tree_str)\n",
        "this_tree.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iosRkDXKdEbs"
      },
      "source": [
        "### Spacy\n",
        "In this secton we use Spacy, a popular natural language processing toolbox, to tokenize and tag a the same sentence. We then look a the stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AA1BOFOdEbv"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(example_text)\n",
        "print(example_text)\n",
        "print()\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "M_gy_-1kdEbw"
      },
      "outputs": [],
      "source": [
        "en_model = spacy.load('en_core_web_sm')\n",
        "#  list of default stop words in spaCy english model\n",
        "stopwords = en_model.Defaults.stop_words\n",
        "stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5WJawMmdEb7"
      },
      "source": [
        "## Embeddings\n",
        "In this section we look at word embeddings and what we can do with them to represent language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "7pzEq8nydEb7"
      },
      "outputs": [],
      "source": [
        "api.info()['models']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd7QAF7jdEb8"
      },
      "outputs": [],
      "source": [
        "# download the model\n",
        "model_glove_twitter = api.load(\"glove-twitter-25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR1HjY22dEb9"
      },
      "outputs": [],
      "source": [
        "model_glove_twitter['cat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOl7n21IdEb9"
      },
      "outputs": [],
      "source": [
        "model_glove_twitter.most_similar(\"cat\",topn=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btp3hGD3dEb-"
      },
      "outputs": [],
      "source": [
        "model_glove_twitter.doesnt_match([\"dog\",\"cat\",\"apple\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "r88xg7vJdEbd"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Data loading and preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-NhluDBdEbX"
      },
      "outputs": [],
      "source": [
        "dataset\n",
        "data = pds.read_csv(filepath_or_buffer=dataset, encoding = \"ISO-8859-1\", on_bad_lines = 'warn')\n",
        "data.loc[31]['tweet']\n",
        "tweets = data['tweet'].tolist()\n",
        "# replace links\n",
        "tweet_normed = [re.sub(r\"http:\\/\\/.*\", \"[link]\", x) for x in tweets]\n",
        "print(tweets[1091])\n",
        "print(tweet_normed[1091])\n",
        "# replace twitter handles\n",
        "tweet_normed = [re.sub(r'@[A-Za-z0-9_-]* ', '[twitter_handle] ', x) for x in tweet_normed]\n",
        "print(tweets[31])\n",
        "print(tweet_normed[31])\n",
        "# remove extra spaces\n",
        "tweet_normed = [re.sub(r\"\\s+\", ' ', x) for x in tweet_normed]\n",
        "print(tweets[31])\n",
        "print(tweet_normed[31])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jFt5w4DFdEbx"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(tweet_normed[31])\n",
        "print(tweet_normed[31])\n",
        "print()\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t29Ev09ZdEby"
      },
      "outputs": [],
      "source": [
        "doc = nlp(tweet_normed[31])\n",
        "print(tweet_normed[31])\n",
        "print()\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyLMd0w2dEby"
      },
      "outputs": [],
      "source": [
        "doc = nlp(tweet_normed[1091])\n",
        "print(tweet_normed[1091])\n",
        "print()\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlE7jEG-dEbr"
      },
      "outputs": [],
      "source": [
        "print(tweet_normed[31])\n",
        "tokenizer.tokenize(tweet_normed[31])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuben8qHdEb0"
      },
      "outputs": [],
      "source": [
        "doc = nlp(tweet_normed[1091].lower())\n",
        "print(tweet_normed[1091].lower())\n",
        "print()\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SJTRELNdEb1"
      },
      "source": [
        "*Not perfect, consider using TweetNLP!*\n",
        "http://www.cs.cmu.edu/~ark/TweetNLP/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPqrx6SddEb2"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBGd3CXJdEb3"
      },
      "outputs": [],
      "source": [
        "#this_tweet = tweets[1061]\n",
        "this_tweet = \"penguins are flightless birds.\"\n",
        "print(this_tweet)\n",
        "tokenized = bert_tokenizer.tokenize(this_tweet)\n",
        "for token in tokenized:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdzUSUbVdEb4"
      },
      "outputs": [],
      "source": [
        "doc = nlp(this_tweet)\n",
        "print(this_tweet)\n",
        "print()\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvICREtmdEb4"
      },
      "source": [
        "## Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvIzq2VidEb5"
      },
      "outputs": [],
      "source": [
        "corpus = [nlp(tweet) for tweet in tweets]\n",
        "tokens = [[token.text for token in doc] for doc in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62bPAu1YdEb5"
      },
      "outputs": [],
      "source": [
        "dct = Dictionary(tokens)\n",
        "docs = [dct.doc2bow(line) for line in tokens]\n",
        "lda = LdaModel(docs, num_topics=15, id2word=dct, passes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "ORtNxtI4dEb6"
      },
      "outputs": [],
      "source": [
        "for topic in range(15):\n",
        "    for x in lda.get_topic_terms(topic, 10):\n",
        "        print(topic, dct[x[0]])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zErMU7LodEb6"
      },
      "outputs": [],
      "source": [
        "idx = 30\n",
        "print(tweets[idx])\n",
        "doc_bow = docs[idx]\n",
        "print(doc_bow)\n",
        "doc_lda = lda[doc_bow]\n",
        "print(doc_lda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRH6hK70dEcA"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWMUGlqTdEcB"
      },
      "outputs": [],
      "source": [
        "inputs = bert_tokenizer(\"penguins are flightless birds.\", return_tensors=\"pt\")\n",
        "outputs = bert(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV55M5SzdEcB"
      },
      "outputs": [],
      "source": [
        "outputs.last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poqL-CasdEcC"
      },
      "outputs": [],
      "source": [
        "outputs.last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp89TIgldEcD"
      },
      "outputs": [],
      "source": [
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMHO9M6FdEcD"
      },
      "outputs": [],
      "source": [
        "for x in inputs['input_ids']:\n",
        "    print(bert_tokenizer.convert_ids_to_tokens(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O79xoPTdEcE"
      },
      "outputs": [],
      "source": [
        "#help(bert_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWLvO4-idEcE"
      },
      "source": [
        "## Small ML example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLBcnk23dEcF"
      },
      "outputs": [],
      "source": [
        "data['tweet_normed'] = tweet_normed\n",
        "\n",
        "yes_tweets = data[data['existence'].isin([\"Yes\", \"Y\"])]['tweet_normed'].tolist()\n",
        "no_tweets = data[data['existence'].isin([\"No\", \"N\"])]['tweet_normed'].tolist()\n",
        "len(yes_tweets),len(no_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMPIrPx-dEcG"
      },
      "outputs": [],
      "source": [
        "# we encode climate deniers as 1\n",
        "import numpy as np\n",
        "def shuffle_lists(a,b):\n",
        "    if len(a)!=len(b):\n",
        "        raise Exception(\"Sorry, list should be of the same length\")\n",
        "    c = list(range(len(a)))\n",
        "    np.random.shuffle(c)\n",
        "    a_shuffled = []\n",
        "    b_shuffled = []\n",
        "    for i in c:\n",
        "        a_shuffled.append(a[i])\n",
        "        b_shuffled.append(b[i])\n",
        "    return a_shuffled, b_shuffled\n",
        "train_set = yes_tweets[:2500]+no_tweets[:950]\n",
        "y_train = [0]*2500+[1]*950\n",
        "\n",
        "train_set, y_train = shuffle_lists(train_set, y_train)\n",
        "\n",
        "test_set = yes_tweets[2500:]+no_tweets[950:]\n",
        "y_test = [0]*611+[1]*164\n",
        "\n",
        "test_set, y_test = shuffle_lists(test_set, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bwKmNvrdEcG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tf = text.TfidfVectorizer()\n",
        "x_train_tfidf = tf.fit_transform(train_set)\n",
        "x_test_tfidf = tf.transform(test_set)\n",
        "print(x_train_tfidf.shape)\n",
        "print(x_test_tfidf.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIxf8R0_dEcH"
      },
      "outputs": [],
      "source": [
        "#estimate sparsity\n",
        "p = 100 * x_train_tfidf.nnz / float(x_train_tfidf.shape[0] * x_train_tfidf.shape[1])\n",
        "print(f\"Each sample has ~{p:.2f}% non-zero features.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAPvTdzIdEcH"
      },
      "outputs": [],
      "source": [
        "#bernouli naive bayes\n",
        "bnb = ms.GridSearchCV(\n",
        "    nb.BernoulliNB(),\n",
        "    param_grid={'alpha': np.logspace(-2., 2., 50)})\n",
        "bnb.fit(x_train_tfidf, y_train)\n",
        "bnb.score(x_test_tfidf, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9_1y2RhdEcJ"
      },
      "outputs": [],
      "source": [
        "#Logistic regression\n",
        "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
        "lrg = ms.GridSearchCV(\n",
        "    LogisticRegression(),\n",
        "    param_grid=grid,scoring='f1_micro')\n",
        "lrg.fit(x_train, y_train)\n",
        "lrg.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRFOOS6LdEcJ"
      },
      "outputs": [],
      "source": [
        "#let's test on a made up sample\n",
        "\n",
        "#test_text = 'The whole climate crisis is not only Fake News, it’s Fake Science. There is no climate crisis, there’s weather and climate all around the world, and in fact carbon dioxide is the main building block of all life. tweeter_hanldle'\n",
        "#test_text = \"Global warming is endangering our livelihoods.\"\n",
        "#test_text = \"The whole country is freezing; this must be due to global warming.\"\n",
        "test_text = \"I don't believe in climate change\"\n",
        "print(test_text)\n",
        "print(\"The output lablel with tf-idf is (0: beleiver 1: denier) {:d}\".format(int(bnb.predict(tf.transform([test_text])))))\n",
        "bert_feat = np.expand_dims(extract_bert([test_text]).cpu().detach().numpy(),axis=0)\n",
        "print(\"The output lablel with distill bert is (0: beleiver 1: denier) {:d}\".format(int(lrg.predict(bert_feat))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3G455TYdEcK"
      },
      "source": [
        "0: beleiver 1: denier"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}