{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "def remove_stopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc))\n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def make_trigrams(texts, trigram_mod):\n",
    "    [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts,\n",
    "                  nlp,\n",
    "                  allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# !wget https://github.com/khider/INF549/tree/master/In-class%20exercises/Practicum-7/20news-bydate_py3.pkz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a corpus of documents and build a dictionary out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "\n",
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = corpora.Dictionary(common_texts)\n",
    "\n",
    "print(list(common_dictionary.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively\n",
    "print(common_dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build BOW representation of corpus of common texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "print(common_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show bow representation of corpus with actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"* BOW corpus with actual words\")\n",
    "id_words = [[(common_dictionary[id], count) for (id, count) in line] for line in common_corpus]\n",
    "print(id_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LDA model with 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus = common_corpus, id2word = common_dictionary, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display learned topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for topic in lda.get_topics():\n",
    "#    print(topic)\n",
    "\n",
    "# display words numerically encoded\n",
    "#for topic in lda.show_topics():\n",
    "#    print(topic)\n",
    "\n",
    "# diplay topics\n",
    "pprint(lda.show_topics(num_words=len(common_dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = [\n",
    "        ['computer', 'time', 'graph'],\n",
    "        ['survey', 'response', 'eps'],\n",
    "        ['human', 'system', 'computer']\n",
    "    ]\n",
    "new_corpus = [common_dictionary.doc2bow(text) for text in new_documents]\n",
    "\n",
    "print(new_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get BOW representation of first new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vector of document\n",
    "bow_doc = common_dictionary.doc2bow(new_documents[0])\n",
    "\n",
    "print(bow_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed document in space: express bow representation as convex combination of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>>> topic prob distribution: lda[bow_doc]\")\n",
    "print(lda[bow_doc])\n",
    "\n",
    "# or\n",
    "# print(lda.get_document_topics(bow_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update LDA model with new texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.update(new_corpus)\n",
    "pprint(lda.show_topics(num_words=len(common_dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Consider new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # to do only once\n",
    "    # import nltk\n",
    "    # nltk.download(\"stopwords\")\n",
    "    if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "            getattr(ssl, '_create_unverified_context', None)):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    # define stop words\n",
    "    stop_words = ['stop', 'the', 'to', 'and', 'a', 'in', 'it',\n",
    "                  'is', 'I', 'that', 'had', 'on', 'for', 'were', 'was',\n",
    "                  'from', 'subject', 're', 'edu', 'use']\n",
    "\n",
    "    # nltk.download('stopwords')\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "    # read corpus\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', data_home=\"./\")\n",
    "    data = newsgroups_train.data\n",
    "    \n",
    "    print(data[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "# it will print the data after prepared for stopwords\n",
    "print(data[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words and lemmatize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data, min_count=5, threshold=100)\n",
    "#trigram = gensim.models.Phrases(bigram[data], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "data_words_nostops = remove_stopwords(data, stop_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops, bigram_mod)\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, nlp, allowed_postags=[\n",
    "    'NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:4])  # it will print the lemmatized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(\"* corpus in BOW format with actual words\")\n",
    "# print(corpus[:4])\n",
    "# it will print the words with their frequencies.\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]])\n",
    "\n",
    "print(\"\\n* Build LDA model\")\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=20,\n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True\n",
    "    )\n",
    "\n",
    "doc_lda = lda_model[corpus]\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "lda_model.save(\"big_lda_model\")\n",
    "\n",
    "# load with\n",
    "# lda = LdaModel.load(\"big_lda_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
