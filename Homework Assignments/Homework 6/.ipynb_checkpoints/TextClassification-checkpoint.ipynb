{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g93n5ysr-vI"
   },
   "source": [
    "# A Notebook for Text Classification #  \n",
    "\n",
    "This notebook will show you how to classify text documents.  Before running a classifier, the text has to be converted into features.  This is called featurization. \n",
    "\n",
    "The following cell contains some predefined functions to implement text featurization and classification. Please make sure you have run this cell before you run other cells in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eeLdi5B_r-vK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def SampleData(dataset):\n",
    "    import pandas as pd\n",
    "    df=pd.read_csv(dataset,'\\t')\n",
    "    with pd.option_context('max_colwidth',160):\n",
    "        display(df.head())\n",
    "    return df.head()\n",
    "    \n",
    "\n",
    "def featDataset(dataset):\n",
    "    output=dataset[:-4]+'_Vectorized.txt'\n",
    "    with open(output,\"w\") as w:\n",
    "        with open(dataset,encoding='utf-8', mode = 'r') as f:\n",
    "            data=f.readlines()\n",
    "            text=[entry.split('\\t')[1].rstrip() for entry in data[1:]]\n",
    "            labels=[entry.split('\\t')[0] for entry in data[1:]]\n",
    "            parsedText=list(map(textParse,text))\n",
    "            vocabList=createVocabList(parsedText)\n",
    "            for word in vocabList:\n",
    "                w.write(word+',')\n",
    "            w.write('class\\n')\n",
    "            for i in range(len(labels)):\n",
    "                returnVec=setOfWords2Vec(vocabList,parsedText[i])\n",
    "                for num in returnVec:\n",
    "                    w.write(str(num)+',')\n",
    "                w.write(labels[i]+\"\\n\")\n",
    "            return vocabList\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet=set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet=vocabSet|set(document)\n",
    "    return list(vocabSet)\n",
    "        \n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    #listOfTokens=re.split(r'\\W*',bigString)\n",
    "    listOfTokens=re.split(r'[^A-Za-z]+',bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok)>2]\n",
    "\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec=[0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "        else: print('the word: %s is not in my Vocabulary' % word)\n",
    "    return returnVec\n",
    "\n",
    "def loadDataSet(dataset): \n",
    "    with open(dataset) as f:\n",
    "        data=f.readlines()\n",
    "        attributes=data[0].rstrip().split(',')[:-1]\n",
    "        #print(\"attributes\",len(attributes))\n",
    "        instances=[entry.rstrip().split(',')[:-1] for entry in data[1:]]\n",
    "        dataArray=[]\n",
    "        for i in range(len(instances[0])):\n",
    "            try:\n",
    "                dataArray.append([float(instance[i]) for instance in instances])\n",
    "            except:\n",
    "                encodedData,codeBook=encode([instance[i] for instance in instances])\n",
    "                dataArray.append(encodedData)\n",
    "                print(attributes[i],': ',list(codeBook.items()))\n",
    "        instances=np.array(dataArray).T\n",
    "        labels=[entry.rstrip().split(',')[-1] for entry in data[1:]]\n",
    "        return instances,labels\n",
    "    \n",
    "def evaluateClf(clf,instances,labels,n_foldCV):\n",
    "    for item in clf:\n",
    "        if type(item).__name__==\"BernoulliNB\":\n",
    "            scores = cross_val_score(item, instances, labels, cv=n_foldCV)\n",
    "            print(\"======BernoulliNB======\")\n",
    "            print(scores)\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "        elif type(item).__name__==\"GaussianNB\":\n",
    "            scores = cross_val_score(item, instances, labels, cv=n_foldCV)\n",
    "            print(\"======GaussianNB======\")\n",
    "            print(scores)\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "        elif type(item).__name__==\"MultinomialNB\":\n",
    "            scores = cross_val_score(item, instances, labels, cv=n_foldCV)\n",
    "            print(\"======MultinomialNB======\")\n",
    "            print(scores)\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "def predict(testset):\n",
    "    if \"clf_G\" in globals():\n",
    "        prediction=clf_G.predict(testset)\n",
    "        print(\"GaussianNB: \",prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8br7Mvj7r-vN"
   },
   "source": [
    "## Explore the data\n",
    "[\"SMSSpamCollection.txt\"](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) includes 5572 SMS messages collected from four different research sources and they were labeled as spam or ham. \"testset_SMS\" is the testset and contains two randomly chosen SMS messages from the SMSSpamCollection dataset and they were deleted from the original dataset to prevent dataset contamination. \n",
    "\n",
    "The example text data that we are using are SMS messages which are labeled as spam or no spam (ham in the file).  The task is to classify a new text message as spam or no spam. Each SMS message is considered as a document (an instance for classification). All the instances are stored in a single file, and each line corresponds to a single document.\n",
    "\n",
    "The following cell will give you an excerpt of the SMS message dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "CbjB2gVqsGAJ",
    "outputId": "384bb873-0b3b-4350-bc59-4d4a441e1079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-26 11:26:12--  https://raw.githubusercontent.com/khider/INF549/master/Homework%20Assignments/Homework%206/SMSSpamCollection.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "ERROR: cannot verify raw.githubusercontent.com's certificate, issued by ‘CN=DigiCert SHA2 High Assurance Server CA,OU=www.digicert.com,O=DigiCert Inc,C=US’:\r\n",
      "  Unable to locally verify the issuer's authority.\r\n",
      "To connect to raw.githubusercontent.com insecurely, use `--no-check-certificate'.\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                       content  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                            U dun say so early hor... U c already then say...  \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/khider/INF549/master/Homework%20Assignments/Homework%206/SMSSpamCollection.txt\n",
    "sample = SampleData('SMSSpamCollection.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgiln6SyxqqS"
   },
   "source": [
    "The cells below display the number of instances in each class in two different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "rIpCaPeVxp_B",
    "outputId": "ec212756-873b-487c-a5ca-1b4db9c3916f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'SMSSpamCollection.txt'\n",
    "df=pd.read_csv(dataset,'\\t')\n",
    "df['class'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qrwiVhaPyZtv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4824\n",
       "spam     746\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16Qs4K0mr-vO"
   },
   "source": [
    "##  An Example of Featurization\n",
    "Before we explore the real data, it would be helpful to understand how featurization works first. The following cell will show you two documents and how they are combined to generate two feature vectors.  \n",
    "\n",
    "Many machine learning algorithms only take numerical data as input. Since our data is words, we need a way to convert it into numerical data. This is called featurization. First, we will create a list of all the words that appear in all the documents. That list will enable us to create a **vector** for each document where each element of the vector corresponds to a word in that list. For a given instance, each element is a 1 if that word is in the instance and a 0 if it is not.  \n",
    " \n",
    "For instance, if the two documents are, **\"I am taking INF549\"** and **\"I love learning data science\"**, then the vocabulary list would be [\"inf\",\"love\",\"learning\",\"taking\",\"science\",\"data\"] and the two vectorized instances would be [1,0,0,1,0,0] and [0,1,1,0,1,1]. 0 and 1 are boolean values denoting the presence of a word (token). 0 means the word doesn't appear and 1 means it appears in the document. **In our algorithm, the fuction only keeps tokens that are made of English characters and whose lengths are longer than 2**. Run the cell below to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "1pO4oQ4ir-vO",
    "outputId": "f4660408-e90f-45b4-b39f-f5024f3d2772"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>inf</th>\n",
       "      <th>taking</th>\n",
       "      <th>learning</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Text 1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Text 2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        science  data  inf  taking  learning  love\n",
       "Text 1        0     0    1       1         0     0\n",
       "Text 2        1     1    0       0         1     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "two_pieces_text=list(map(textParse,[\"I am taking INF549\",\"I love learning data science\"]))\n",
    "vocabList=createVocabList(two_pieces_text)\n",
    "featurized_vectors=pd.DataFrame([setOfWords2Vec(vocabList,instance) for instance in two_pieces_text],columns=vocabList,\\\n",
    "                               index=[\"Text 1\",\"Text 2\"])\n",
    "display(featurized_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aByj2vWTr-vP"
   },
   "source": [
    "## Preprocess the document to identify all the words\n",
    "Before we featurize the text, we have to create a list with all the words that appear in all the messages. The following cell will generate and display the vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYBUjsvTr-vP",
    "outputId": "7eae91fc-d0e5-4aa5-8e28-a53599824f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary List: \n",
      " ['goes', 'final', 'win', 'txt', 'lives', 'free', 'rate', 'there', 'dun', 'crazy', 'apply', 'bugis', 'great', 'lar', 'say', 'entry', 'cup', 'early', 'cine', 'question', 'wif', 'std', 'receive', 'usf', 'wat', 'think', 'jurong', 'tkts', 'amore', 'then', 'comp', 'wkly', 'point', 'over', 'oni', 'around', 'buffet', 'already', 'may', 'joking', 'though', 'only', 'hor', 'text', 'got', 'nah', 'don', 'here', 'available', 'world', 'until']\n"
     ]
    }
   ],
   "source": [
    "text=[instance.rstrip() for instance in sample.iloc[:,1]]\n",
    "parsedText=list(map(textParse,text))\n",
    "vocabList=createVocabList(parsedText)\n",
    "#print('Parsed Text: ')\n",
    "#for instance in parsedText:\n",
    "#    print(instance)\n",
    "print('Vocabulary List: \\n',vocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjgmMSuNr-vQ"
   },
   "source": [
    "## Generate features for the documents##\n",
    "For each document, we generate a feature vector which records the appearance of each word in the vocabulary list. \n",
    "\n",
    "The following cell will output the vectors corresponding to the parsed text you got from the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7lJsoxkr-vQ",
    "outputId": "16857c95-9a84-462b-ad2d-a659a94072f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for instance in parsedText:\n",
    "    print(setOfWords2Vec(vocabList,instance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2V79gORr-vQ"
   },
   "source": [
    "## Put them together\n",
    "Now we will generate feature vectors for each of the documents to the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "DCkxLHtYr-vR"
   },
   "source": [
    "**Training set vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGKOdhqor-vR",
    "outputId": "f6e08532-ccf8-4bcc-f261-e59960292ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The featurization of your documents is done!\n"
     ]
    }
   ],
   "source": [
    "dataset='SMSSpamCollection.txt'\n",
    "vocabList=featDataset(dataset)\n",
    "print('The featurization of your documents is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPTal0FMr-vR"
   },
   "source": [
    "## Training and testing a Naïve Bayes Classifier\n",
    "The following cells will train a Naïve Bayes classifier on the featurized dataset. There are three Naive Bayes classifiers provided and calculate the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PHUGm5hEr-vT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data prep\n",
    "instances,labels=loadDataSet(dataset[:-4]+'_Vectorized.txt')\n",
    "idx_spam = [i for i, j in enumerate(labels) if j == 'spam']\n",
    "idx_ham = [i for i, j in enumerate(labels) if j == 'ham']\n",
    "Xs_Train, Xs_test, Ys_train, Ys_test = train_test_split(instances[idx_spam], np.array(labels)[idx_spam], test_size=0.25, random_state=42) \n",
    "Xh_Train, Xh_test, Yh_train, Yh_test = train_test_split(instances[idx_ham], np.array(labels)[idx_ham], test_size=0.25, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2-5xSrdX6XqG"
   },
   "outputs": [],
   "source": [
    "X_train=np.append(Xs_Train,Xh_Train, axis=0)\n",
    "X_test=np.append(Xs_test,Xh_test, axis=0) \n",
    "Y_train=np.append(Ys_train,Yh_train) \n",
    "Y_test=np.append(Ys_test,Yh_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bXiKBoR1Vb6",
    "outputId": "e72597dd-9da5-4f97-b20a-0788cdf42957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes is used.\n"
     ]
    }
   ],
   "source": [
    "clf_G = MultinomialNB()\n",
    "clf_G.fit(X_train, Y_train)\n",
    "print(\"Multinomial Naive Bayes is used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6FI4PqS_E-W",
    "outputId": "458ecede-98d8-463c-a02a-5565ab2a7286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8916786226685797"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction=clf_G.predict(X_test)\n",
    "accuracy_score(Y_test,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0MHP51Kr-vU"
   },
   "source": [
    "## Predict unseen examples\n",
    "The following cell will ask you to input the test set, featurize it and predict it. Run the cell and you will get the results from the classifier you have trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a80VPQDSr-vU"
   },
   "source": [
    "**Test set vectorization**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ul4jmI28r-vU",
    "outputId": "e501c337-4b7d-45d4-af52-663e831ef9fb"
   },
   "outputs": [],
   "source": [
    "testset=input('Please enter the text message that you want to test:')\n",
    "returnVec=setOfWords2Vec(vocabList,textParse(testset))\n",
    "print(returnVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF-7t7AKr-vU"
   },
   "source": [
    "**Predict result**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Hl1N4-Nr-vV",
    "outputId": "48046021-ed37-469c-b1da-3ac5d6ee631a"
   },
   "outputs": [],
   "source": [
    "testset=np.array(returnVec).reshape(1, -1)\n",
    "predict(testset)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
